# backend/main.py
from fastapi import FastAPI, Request
from fastapi.responses import HTMLResponse, JSONResponse
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
from jinja2 import Environment, FileSystemLoader
import json
import asyncio
from dotenv import load_dotenv
from pathlib import Path
import uvicorn
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# å¯¼å…¥æ•°æ®åº“æ“ä½œ
import database

BASE_DIR = Path(__file__).resolve().parent.parent  #ä¸»æ–‡ä»¶å¤¹åœ°å€
FRONTEND_DIR = BASE_DIR / "frontend"

app = FastAPI()
load_dotenv(dotenv_path= BASE_DIR / r"backend\MyAI.env")

templates = Jinja2Templates(directory=str(FRONTEND_DIR))

# æŒ‚è½½é™æ€èµ„æº
app.mount("/static", StaticFiles(directory=str(FRONTEND_DIR)), name="static")

# Jinja2 æ¨¡æ¿å¼•æ“ï¼ˆç”¨äºæ¸²æŸ“ HTMLï¼‰
template_env = Environment(loader=FileSystemLoader(str(FRONTEND_DIR)))

# å…¨å±€å˜é‡ï¼ˆåœ¨ app å¯åŠ¨æ—¶åŠ è½½ä¸€æ¬¡ï¼‰
model = None
tokenizer = None

def load_model():
    """åŒæ­¥å‡½æ•°ï¼šå®é™…åŠ è½½æ¨¡å‹"""
    model_name = str(BASE_DIR / "model\deepseek-coder-1.3b-instruct")
    print("Loading tokenizer...")
    tok = AutoTokenizer.from_pretrained(model_name)
    print("Loading model...")
    m = AutoModelForCausalLM.from_pretrained(
        model_name,
        dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
        device_map="auto",
        low_cpu_mem_usage=True
    ).eval()
    return m, tok

async def start_load():
    global model,tokenizer
    """åœ¨ç¨‹åºå¯åŠ¨æ—¶ï¼Œéé˜»å¡åœ°åŠ è½½æ¨¡å‹"""
    loop = asyncio.get_event_loop()
    # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡ŒåŒæ­¥åŠ è½½ï¼Œé¿å…é˜»å¡äº‹ä»¶å¾ªç¯
    model, tokenizer = await loop.run_in_executor(None, load_model)
    print("âœ… Model loaded during startup!")

def shutdown_event():
    """åœ¨åº”ç”¨å…³é—­æ—¶æ¸…ç†èµ„æº"""
    global model, tokenizer
    if model is not None:
        del model
    if tokenizer is not None:
        del tokenizer
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    print("ğŸ‘‹ Cleaned up model and CUDA cache on shutdown.")

app.add_event_handler("startup",start_load)
app.add_event_handler("shutdown",shutdown_event)

@app.get("/", response_class=HTMLResponse)
async def home(): #æŒ‚è½½ç½‘é¡µï¼Œè¿æ¥å‰ç«¯
    template = template_env.get_template("myapp.html")
    characters = database.get_all_characters()
    characters_json = json.dumps([{ "id": c["id"], "name": c["name"], "trait": c["trait"] } for c in characters], ensure_ascii=False)
    content = template.render(characters=characters, characters_json=characters_json)
    return HTMLResponse(content=content)

@app.post("/chat")
async def deepseek_chat(request: Request):
    data = await request.json()
    character_id = data.get("character_id")
    user_message = data.get("message")

    # æŸ¥è¯¢è§’è‰²ä¿¡æ¯
    conn = database.get_connection()
    cursor = conn.cursor(dictionary=True)
    cursor.execute("SELECT name, trait FROM characters WHERE id = %s", (character_id,))
    character = cursor.fetchone()
    conn.close()

    if not character:
        return JSONResponse({"error": "è§’è‰²ä¸å­˜åœ¨"}, status_code=404)

    # æ„å»ºç³»ç»Ÿæç¤ºè¯
    system_prompt = f"""
ä½ æ­£åœ¨æ‰®æ¼” {character['name']}ã€‚
äººç‰©è®¾å®šï¼š{character['trait']}
è¯·å§‹ç»ˆä»¥è¿™ä¸ªè§’è‰²çš„èº«ä»½ã€è¯­æ°”å’Œæ€ç»´æ–¹å¼å›ç­”é—®é¢˜ã€‚
ä¸è¦æš´éœ²ä½ æ˜¯ AI çš„äº‹å®ã€‚
""".strip()

    # ğŸ‘‡ å…³é”®ï¼šä½¿ç”¨æœ¬åœ°æ¨¡å‹è¿›è¡Œæ¨ç†
    try:
        # ç¡®ä¿æ¨¡å‹å·²åŠ è½½
        global model, tokenizer
        if model is None or tokenizer is None:
            return JSONResponse({"error": "æ¨¡å‹å°šæœªåŠ è½½ï¼Œè¯·å…ˆå¯åŠ¨æ¨¡å‹"}, status_code=500)

        # æ„é€ å¯¹è¯å†å²ï¼ˆå¿…é¡»ä½¿ç”¨ chat templateï¼‰
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_message}
        ]

        # ä½¿ç”¨ tokenizer.apply_chat_template æ„é€ è¾“å…¥æ–‡æœ¬
        input_text = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True  # è®©æ¨¡å‹çŸ¥é“è¦å¼€å§‹ç”Ÿæˆ assistant å›å¤   
        )

        # Tokenize
        inputs = tokenizer(input_text, return_tensors="pt").to(model.device)

        # ç”Ÿæˆå‚æ•°
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=512,
                temperature=0.85,
                top_p=0.95,
                do_sample=True,
                repetition_penalty=1.1,
                eos_token_id=tokenizer.eos_token_id,
                pad_token_id=tokenizer.eos_token_id  # é¿å… decoder-only æ¨¡å‹ padding æŠ¥é”™
            )

        # è§£ç è¾“å‡ºï¼ˆå»æ‰è¾“å…¥éƒ¨åˆ†ï¼‰
        full_response = tokenizer.decode(outputs[0], skip_special_tokens=False)
        
        # æå– assistant çš„å›å¤å†…å®¹
        # æ³¨æ„ï¼šapply_chat_template å·²ç»æ·»åŠ äº† <|assistant|> æ ‡è®°
        if "<|assistant|>" in full_response:
            reply = full_response.split("<|assistant|>")[-1].strip()
        else:
            reply = full_response[len(input_text):].strip()

        # æ¸…ç†ç»“å°¾å¯èƒ½çš„æ— å…³ token
        eot_token = "<|EOT|>"
        if eot_token in reply:
            reply = reply.split(eot_token)[0].strip()

        # ä¿å­˜å¯¹è¯è®°å½•
        database.save_conversation(character_id, user_message, reply)

        return JSONResponse({"reply": reply})

    except Exception as e:
        import traceback
        error_msg = traceback.format_exc()  # æ›´è¯¦ç»†çš„é”™è¯¯æ—¥å¿—
        return JSONResponse({"error": f"æ¨ç†å¤±è´¥: {str(e)}", "detail": error_msg}, status_code=500)

# éllama.cppå®ç°

if __name__ == "__main__":
    uvicorn.run("myapp:app", host="127.0.0.1", port=8000, reload=True)