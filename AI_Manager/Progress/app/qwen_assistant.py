"""
GPT-4 APIé›†æˆæ¨¡å— â†’ å·²æ”¹é€ ä¸ºã€é€šä¹‰åƒé—® Qwenã€‘APIé›†æˆæ¨¡å—
ç”¨äºæ„å›¾ç†è§£å’Œä»»åŠ¡å¤„ç†
"""
import json
import re
from config import DASHSCOPE_API_KEY, DASHSCOPE_MODEL  # ç¡®ä¿ config.py ä¸­æœ‰è¿™ä¸¤ä¸ªé…ç½®
import dashscope
from dashscope import Generation


class QWENAssistant:
    def __init__(self):
        # è®¾ç½® DashScope API Keyï¼ˆâœ… æ›¿ä»£ OpenAI çš„ api_keyï¼‰
        dashscope.api_key = DASHSCOPE_API_KEY

        # æ¨¡å‹åç§°ï¼ˆå¦‚ qwen-max, qwen-plus, qwen-turboï¼‰
        self.model_name = DASHSCOPE_MODEL or 'qwen-max'

        # å¯¹è¯å†å²è®°å½•
        self.conversation_history = []

        # ğŸ”§ ç³»ç»Ÿæç¤ºè¯ï¼šå®šä¹‰ AI åŠ©æ‰‹çš„è§’è‰²å’Œè¾“å‡ºæ ¼å¼ï¼ˆä¿æŒä¸å˜ï¼‰
        self.system_prompt = """
ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½è¯­éŸ³æ§åˆ¶åŠ©æ‰‹ï¼Œèƒ½å¤Ÿç†è§£ç”¨æˆ·çš„è¯­éŸ³æŒ‡ä»¤å¹¶æ‰§è¡Œç›¸åº”çš„ä»»åŠ¡ã€‚

ä½ çš„ä¸»è¦èƒ½åŠ›åŒ…æ‹¬ï¼š
1. æ’­æ”¾éŸ³ä¹å’Œæ§åˆ¶åª’ä½“
2. æ–‡ä»¶æ“ä½œï¼ˆåˆ›å»ºã€è¯»å–ã€ç¼–è¾‘æ–‡ä»¶ï¼‰
3. æ–‡æœ¬ç”Ÿæˆï¼ˆå†™æ–‡ç« ã€æ€»ç»“ã€ç¿»è¯‘ç­‰ï¼‰
4. ç³»ç»Ÿæ§åˆ¶ï¼ˆæ‰“å¼€åº”ç”¨ã€è®¾ç½®æé†’ç­‰ï¼‰
5. å¤šæ­¥éª¤ä»»åŠ¡ç¼–æ’

å½“ç”¨æˆ·å‘å‡ºæŒ‡ä»¤æ—¶ï¼Œä½ éœ€è¦ï¼š
1. ç†è§£ç”¨æˆ·çš„æ„å›¾
2. ç¡®å®šéœ€è¦æ‰§è¡Œçš„å…·ä½“æ“ä½œ
3. è¿”å›ç»“æ„åŒ–çš„å“åº”ï¼ŒåŒ…å«æ“ä½œç±»å‹å’Œå‚æ•°

ğŸ¯ å“åº”æ ¼å¼å¿…é¡»æ˜¯ä¸¥æ ¼åˆæ³•çš„ JSONï¼š
{
    "intent": "æ“ä½œç±»å‹",
    "action": "å…·ä½“åŠ¨ä½œ",
    "parameters": {"å‚æ•°å": "å‚æ•°å€¼"},
    "response": "ç»™ç”¨æˆ·çš„å›å¤",
    "needs_confirmation": true/false
}

ğŸ“Œ æ”¯æŒçš„æ“ä½œç±»å‹ï¼š
- music: éŸ³ä¹ç›¸å…³æ“ä½œ
- file: æ–‡ä»¶æ“ä½œ
- text: æ–‡æœ¬ç”Ÿæˆ
- system: ç³»ç»Ÿæ§åˆ¶
- task: å¤šæ­¥éª¤ä»»åŠ¡
- chat: æ™®é€šå¯¹è¯

â—è¯·å§‹ç»ˆç”¨ä¸­æ–‡å›å¤ç”¨æˆ·ã€‚
"""

    def process_voice_command(self, voice_text):
        """å¤„ç†è¯­éŸ³æŒ‡ä»¤ï¼Œè¿”å›ç»“æ„åŒ– JSON å“åº”"""
        if not voice_text.strip():
            return self._create_response("chat", "empty", {}, "æˆ‘æ²¡æœ‰å¬æ¸…æ¥šï¼Œè¯·é‡æ–°è¯´è¯ã€‚", False)

        # æ·»åŠ ç”¨æˆ·è¾“å…¥åˆ°å¯¹è¯å†å²
        self.conversation_history.append({"role": "user", "content": voice_text})

        try:
            # æ„å»ºæ¶ˆæ¯åˆ—è¡¨ï¼šç³»ç»Ÿæç¤º + æœ€è¿‘æœ€å¤š10è½®å¯¹è¯ï¼ˆé™åˆ¶ä¸Šä¸‹æ–‡é•¿åº¦ï¼‰
            messages = [{"role": "system", "content": self.system_prompt}]
            messages.extend(self.conversation_history[-10:])  # ä¿ç•™æœ€è¿‘å¯¹è¯

            # ğŸš€ è°ƒç”¨é€šä¹‰åƒé—®æ¨¡å‹ï¼ˆâœ… æ›¿ä»£ openai.ChatCompletion.createï¼‰
            response = Generation.call(
                model=self.model_name,
                messages=messages,
                temperature=0.5,
                top_p=0.8,
                max_tokens=1024
            )

            # âœ… ä¿®æ”¹ç‚¹1ï¼šä¸å†ä½¿ç”¨ .choices[0].message.contentï¼ˆOpenAI ç‰¹æœ‰ï¼‰
            if response.status_code != 200:
                print(f"[é”™è¯¯] Qwen API è°ƒç”¨å¤±è´¥: {response.status_code}, {response.message}")
                return self._create_response("chat", "error", {}, f"æœåŠ¡æš‚æ—¶ä¸å¯ç”¨: {response.message}", False)

            # âœ… æ­£ç¡®è·å–è¾“å‡ºå†…å®¹ï¼ˆé€šä¹‰åƒé—®ä¸“ç”¨ï¼‰
            ai_response = response.output['text'].strip()
            print(f"[DEBUG] æ¨¡å‹åŸå§‹è¾“å‡º: {ai_response}")  # è°ƒè¯•ç”¨

            # å°†æ¨¡å‹å›å¤åŠ å…¥å¯¹è¯å†å²
            self.conversation_history.append({"role": "assistant", "content": ai_response})

            # å°è¯•è§£æ JSON å“åº”
            try:
                parsed_response = json.loads(ai_response)
                return parsed_response
            except json.JSONDecodeError:
                # å¦‚æœä¸æ˜¯å®Œæ•´ JSONï¼Œå°è¯•æå–å¤§æ‹¬å·å†…çš„éƒ¨åˆ†
                json_match = re.search(r'\{[\s\S]*\}', ai_response)  # åŒ¹é…ç¬¬ä¸€ä¸ªå®Œæ•´çš„ { ... }
                if json_match:
                    try:
                        parsed_response = json.loads(json_match.group())
                        return parsed_response
                    except json.JSONDecodeError:
                        pass

                # è‹¥ä»æ— æ³•è§£æï¼Œä½œä¸ºæ™®é€šèŠå¤©è¿”å›
                return self._create_response("chat", "reply", {}, ai_response, False)

        except Exception as e:
            print(f"[å¼‚å¸¸] GPT APIè°ƒç”¨é”™è¯¯: {e}")
            return self._create_response("chat", "error", {}, "æŠ±æ­‰ï¼Œæˆ‘é‡åˆ°äº†ä¸€äº›æŠ€æœ¯é—®é¢˜ï¼Œè¯·ç¨åå†è¯•ã€‚", False)

    def _create_response(self, intent, action, parameters, response, needs_confirmation):
        """åˆ›å»ºæ ‡å‡†å“åº”æ ¼å¼"""
        return {
            "intent": intent,
            "action": action,
            "parameters": parameters,
            "response": response,
            "needs_confirmation": needs_confirmation
        }

    # -----------------------------
    # âœ… æ‰€æœ‰ç”Ÿæˆç±»æ–¹æ³•ä¹Ÿæ”¹ä¸ºä½¿ç”¨ Qwen
    # -----------------------------

    def generate_text(self, prompt, task_type="general"):
        """ç”ŸæˆæŒ‡å®šç±»å‹çš„æ–‡æœ¬å†…å®¹"""
        try:
            system_prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æœ¬ç”ŸæˆåŠ©æ‰‹ã€‚æ ¹æ®ç”¨æˆ·çš„è¦æ±‚ç”Ÿæˆé«˜è´¨é‡çš„æ–‡æœ¬å†…å®¹ã€‚

ä»»åŠ¡ç±»å‹ï¼š{task_type}
è¦æ±‚ï¼š{prompt}

è¯·ç”Ÿæˆç›¸åº”çš„æ–‡æœ¬å†…å®¹ï¼Œç¡®ä¿å†…å®¹å‡†ç¡®ã€æœ‰é€»è¾‘ã€è¯­è¨€æµç•…ã€‚
"""
            response = Generation.call(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.8,
                max_tokens=2000
            )

            if response.status_code == 200:
                return response.output['text']
            else:
                print(f"æ–‡æœ¬ç”Ÿæˆå¤±è´¥: {response.message}")
                return f"ç”Ÿæˆå¤±è´¥: {response.message}"

        except Exception as e:
            print(f"æ–‡æœ¬ç”Ÿæˆé”™è¯¯: {e}")
            return f"æŠ±æ­‰ï¼Œç”Ÿæˆæ–‡æœ¬æ—¶é‡åˆ°é”™è¯¯ï¼š{str(e)}"

    def summarize_text(self, text):
        """æ€»ç»“æ–‡æœ¬å†…å®¹"""
        try:
            prompt = f"è¯·æ€»ç»“ä»¥ä¸‹æ–‡æœ¬çš„ä¸»è¦å†…å®¹ï¼š\n\n{text}"
            response = Generation.call(
                model=self.model_name,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,
                max_tokens=500
            )
            if response.status_code == 200:
                return response.output['text']
            else:
                return f"æ€»ç»“å¤±è´¥: {response.message}"
        except Exception as e:
            print(f"æ–‡æœ¬æ€»ç»“é”™è¯¯: {e}")
            return f"æŠ±æ­‰ï¼Œæ€»ç»“æ–‡æœ¬æ—¶é‡åˆ°é”™è¯¯ï¼š{str(e)}"

    def translate_text(self, text, target_language="è‹±æ–‡"):
        """ç¿»è¯‘æ–‡æœ¬"""
        try:
            prompt = f"è¯·å°†ä»¥ä¸‹æ–‡æœ¬ç¿»è¯‘æˆ{target_language}ï¼š\n\n{text}"
            response = Generation.call(
                model=self.model_name,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,
                max_tokens=1000
            )
            if response.status_code == 200:
                return response.output['text']
            else:
                return f"ç¿»è¯‘å¤±è´¥: {response.message}"
        except Exception as e:
            print(f"æ–‡æœ¬ç¿»è¯‘é”™è¯¯: {e}")
            return f"æŠ±æ­‰ï¼Œç¿»è¯‘æ–‡æœ¬æ—¶é‡åˆ°é”™è¯¯ï¼š{str(e)}"

    def clear_history(self):
        """æ¸…ç©ºå¯¹è¯å†å²"""
        self.conversation_history.clear()


# =============================
# ğŸ§ª æµ‹è¯•ä»£ç 
# =============================
if __name__ == "__main__":
    assistant = QWENAssistant()

    test_commands = [
        "æ’­æ”¾å‘¨æ°ä¼¦çš„æ­Œæ›²",
        "å†™ä¸€ç¯‡å…³äºæ°”å€™å˜åŒ–çš„æ–‡ç« ",
        "æŠŠè¿™æ®µè¯ç¿»è¯‘æˆè‹±æ–‡ï¼šä»Šå¤©å¤©æ°”çœŸå¥½",
        "æ€»ç»“ä¸€ä¸‹äººå·¥æ™ºèƒ½çš„å‘å±•å†ç¨‹",
        "ä½ å¥½å•Š",
        "æ‰“å¼€æµè§ˆå™¨"
    ]

    for cmd in test_commands:
        print(f"\nğŸ”Š ç”¨æˆ·æŒ‡ä»¤: {cmd}")
        result = assistant.process_voice_command(cmd)
        print("ğŸ¤– AIå“åº”:")
        print(json.dumps(result, ensure_ascii=False, indent=2))
